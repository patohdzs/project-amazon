{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Fvf20OgS8u"
      },
      "source": [
        "# Carbon prices and forest preservation over space and time in the Brazilian Amazon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3tc7UuOW4zp",
        "outputId": "5ef3008d-0495-48be-df23-fe287fc59e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing MCMC Module: mcmc_sampling_original.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import pickle\n",
        "import casadi\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "from mcmc.hmc import create_hmc_sampler\n",
        "from services.data_service import load_site_data\n",
        "\n",
        "# Local Debugging flag; remove when all tested\n",
        "_DEBUG = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkdacge-XQmn"
      },
      "source": [
        "# 1 Parameter Uncertainty\n",
        "\n",
        "# 1.1 Max-min problem\n",
        "We investigate a static formulation of robustness to parameter uncertainty.  For each site, we consider the parameter pair $\\beta^i = (\\gamma^i, \\theta^i)$ for $i=1,2,...,I$, where $θ_i$ is a site-specific productivity parameter, $γ^i \\geq 0$ denotes\n",
        "the density of CO2e that is present in a primary forest in site $i$. Let\n",
        "$\\mathbf{\\beta}$ denote full parameter vector including all sites and hence of dimension $2 \\times I.$\n",
        "\n",
        "Our planner is uncertain about these parameter values and instead has baseline\n",
        "probability distribution $\\pi.$  In addition, this planner is uncertain about what  distribution to use and instead  thinks of $\\pi$ as a rough approximation.  We address this uncertainty by\n",
        "introducing ambiguity about the parameter distribution.\n",
        "\n",
        "Let $d$ be the vector of decisions and $f( \\mathbf{d},\\mathbf{\\beta})$ for the resulting value  given the unknown parameter $\\mathbf{\\beta}.$  We use a divergence measure to capture  ambiguity about the parameter distribution.  For $\\int g( \\mathbf{\\beta}) d\\pi( \\mathbf{\\beta}) = 1,$ the relative entropy (or Kullback-Leibler) divergence\n",
        "\n",
        "$$\n",
        "\\int_{\\mathcal B} \\log g(\\mathbf{\\beta}) g( \\mathbf{\\beta}) d \\pi(\\mathbf{ \\beta}) \\ge 0,\n",
        "$$\n",
        "\n",
        "is a commonly used measure of divergence between a probability $g(\\beta)  d\\pi(  \\beta)$ and the baseline $d\\pi(\\beta).$\n",
        " To produce optimal controls that are robust to the parameter uncertainty, solve\n",
        "\n",
        "$$\n",
        "\\max_{ d} \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi( \\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g( \\mathbf{\\beta}) g(\\mathbf{ \\beta} ) d\\pi( \\mathbf{\\beta})\n",
        "$$\n",
        "\n",
        "where $\\xi > 0$ is penalty parameter. Alternatively, we can think of $\\xi$ a as Lagrange multiplier on a relative entropy divergence constraint.\n",
        "\n",
        "We implement a full commitment to the baseline distribution by  making $\\xi$  arbitrarily large.  More modest settings capture a concern for robustness.\n",
        "\n",
        "(WE SHOULD WRITE THE CODE SOLVE SEPERATELY THE MAX AND THE MIN PROBLEM)\n",
        "\n",
        "## 1.2 Worst case probability\n",
        "\n",
        "One nice feature of using relative entropy divergence to explore distributional sensitivity is that the minimization problem has a quasi-analytical solution:\n",
        "\n",
        "$$\n",
        "-\\xi \\log \\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{ d},  \\mathbf{ \\beta} )  \\right]  d\\pi( \\beta) = \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi(\\mathbf {\\beta})\n",
        "+ \\xi \\int \\log g(\\mathbf{ \\beta}) g( \\mathbf{\\beta}) d\\pi( \\mathbf{\\beta})\\tag{1}\n",
        "$$\n",
        "\n",
        "where the minimizing $g$ given by:\n",
        "\n",
        "$$\n",
        "g^* = \\frac {{\\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta} )\\right]}}\n",
        "{{\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d}, \\tilde  {\\mathbf{\\beta}})\\right]d\\pi( \\tilde {\\mathbf{\\beta}} )}}\\tag{2}\n",
        "$$\n",
        "\n",
        "This tilts the distribution towards smaller objectives for each given decision $\\mathbf{d}$. The candidate solution presumes that:\n",
        "\n",
        "$$\n",
        "\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta})\\right]d\\pi(\\mathbf{\\beta} ) < \\infty\n",
        "$$\n",
        "\n",
        "which implicitly limits how fat the tail can be for the distribution $\\pi$.\n",
        "\n",
        "For conceptual reasons, we also switch the order of the maximization and minimization.  Under quite general conditions, we can invoke the min-max theorem:\n",
        "\n",
        "$$\n",
        "\\min_{g, \\int g d\\pi = 1} \\max_{\\mathbf{d}}  \\int_{\\mathcal B} f(\\mathbf{d},  \\mathbf{\\beta} )   g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g(\\mathbf{\\beta}) g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta})\n",
        "$$\n",
        "where $\\xi > 0$ is penalty parameter.\n",
        "\n",
        "Consider the inner maximization problem:\n",
        "\n",
        "$$\n",
        "\\max_{\\mathbf{d}}  \\int_{\\mathcal B} f(\\mathbf{d},  \\mathbf{\\beta} )   g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta})\n",
        "$$\n",
        "\n",
        "where we are free to drop the relative entropy penalty as it does not depend on the decision $\\mathbf{d}$.  Provided that this problem has a solution for the outer $g$ minimization, then the planner is maximizing against this particular (penalized) \"worst case probability.\"\n",
        "\n",
        "This computation is of interest as a way to interpret the consequences of any given choice of the penalty parameter $\\xi$.  In practice, we find it revealing to explore alternative choices of $\\xi$ and deduce their implications for the implied worst case probabilities. This follows a common practice for robust Bayesian methods.\n",
        "\n",
        "## 1.3 Numerical Solution\n",
        "For solving the robust problem numerically, we take an iterative approach, also supported by the min-max theorem.\n",
        "\n",
        "Specifically, we proceed as follows:\n",
        "\n",
        "1. Given a $g$, we solve the maximization problem for a candidate $\\mathbf{d}.$  We ignore the relative entropy penalty term in this solution.\n",
        "\n",
        "$$\n",
        "\\max_{ d}\\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi( \\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g( \\mathbf{\\beta}) g(\\mathbf{ \\beta} ) d\\pi( \\mathbf{\\beta}),\\text{ given }g\n",
        "$$\n",
        "\n",
        "2. For a given $\\mathbf{d}$, we solve the minimization problem with the relative entropy term to obtain a new candidate for $g$.\n",
        "\n",
        "$$\n",
        " \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi( \\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g( \\mathbf{\\beta}) g(\\mathbf{ \\beta} ) d\\pi( \\mathbf{\\beta}), \\text{ given }\\mathbf{ d}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "g^* = \\frac {{\\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta} )\\right]}}\n",
        "{{\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d}, \\tilde  {\\mathbf{\\beta}})\\right]d\\pi( \\tilde {\\mathbf{\\beta}} )}}\n",
        "$$\n",
        "3. We repeat the steps until we achieve convergence.\n",
        "\n",
        "For step 2, we use a Hamiltonian simulation approach along with the quasi-analytical solution in computing equation [2].  A numerical method is necessary because of the denominator term, and Markov simulation gives us one way to explore a parameter space of potentially large dimension.  Very similar to how a Metropolis-Hastings algorithm can be used  to compute a Bayesian posterior in terms of a prior and a likelihood, we calculate the exponentially tilted solution using $d\\pi(\\mathbf{\\beta})$ and $\\exp\\left[-{\\frac 1 \\xi} f(\\mathbf{d}, \\mathbf{\\beta})\\right].$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7qTpY1y4KDT"
      },
      "source": [
        "# 2 Optimization Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OtkIk3XXZFk"
      },
      "source": [
        "## 2.1 Load Site Data Function\n",
        "\n",
        "The function `load_site_data(site_num, norm_fac=1.0)` is designed to load and preprocess site-specific data, including the prior for $\\beta^i = (\\gamma^i, \\theta^i)$. The function takes two parameters:\n",
        "\n",
        "- `site_num` : An integer representing the site number.\n",
        "- `norm_fac` : A float representing the normalization factor. Its default value is 1.0.\n",
        "\n",
        "The function starts by reading a CSV file specific to the site number passed as the `site_num` parameter. The file should be named as \"calibration_{n}SitesModel.csv\", where `{n}` is the site number.\n",
        "\n",
        "The function then extracts information from several columns in the data frame. The columns are named using the site number and the corresponding information type (e.g., `z_2017_{n}Sites`, `zbar_2017_{n}Sites`, etc.).\n",
        "\n",
        "The extracted information includes:\n",
        "\n",
        "- `z_2017` : Data specific to the site from 2017.\n",
        "- `zbar_2017` : Mean data specific to the site from 2017.\n",
        "- `gamma` : Gamma value for the site.\n",
        "- `gammaSD` : Standard deviation of the gamma value for the site.\n",
        "- `forestArea_2017_ha` : Forest area for the site in 2017 (in hectares).\n",
        "- `theta` : Theta value for the site.\n",
        "\n",
        "After extracting the data, the function normalizes `zbar_2017` and `z_2017` using the normalization factor `norm_fac`.\n",
        "\n",
        "Finally, the function returns a tuple containing the normalized `zbar_2017`, `gamma`, `gammaSD`, `z_2017`, `forestArea_2017_ha`, and `theta` data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EirRY4AXXWe",
        "outputId": "90676f36-95e0-4c22-c458-666b83816593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data successfully loaded from '/Users/patriciohernandez/Projects/project-amazon/data/calibration/25SitesModel/calibration_25SitesModel.csv'\n",
            "gamma [518.85149136 487.54323455 537.7425627  611.75292374 698.1420002\n",
            " 595.09347375 541.33734825 502.06796332 587.9661928  625.60968955\n",
            " 477.07182557 211.93209329 736.55752096 643.36914904 569.0201995\n",
            " 621.07643451 574.72293378 380.3600162  805.56732032 657.29084041\n",
            " 581.51465738 460.26777591 369.3389741  342.95464157 293.63361683]\n",
            "gammaSD [22.65390799 13.62725374 21.34917569 33.30934455 32.70939545 20.61563522\n",
            " 10.31888153  7.7556238  14.95505821 17.15621351 13.73008142 10.07743264\n",
            " 26.61126081 15.76453451  9.326237   20.44218638 20.48965668 14.10450348\n",
            " 34.62746663 16.33038484  7.81833512  0.70327085  2.24782677 13.9507146\n",
            " 19.86937446]\n",
            "z_2017 [5.77092940e-07 5.58918518e-06 7.87674083e-04 9.64686174e-06\n",
            " 1.28634503e-04 1.62486537e-05 7.71270596e-05 4.13615376e-04\n",
            " 1.41563731e-03 2.97549040e-03 7.91141025e-03 3.11951500e-04\n",
            " 4.91859804e-04 5.48889538e-04 1.19367249e-03 1.41661798e-03\n",
            " 6.05514368e-03 6.74644202e-03 8.44104902e-05 2.44820053e-03\n",
            " 7.38590342e-03 8.45649924e-03 5.22554843e-03 8.60690505e-04\n",
            " 2.31193733e-03]\n",
            "gamma - gammaSD [496.19758337 473.91598081 516.393387   578.44357918 665.43260475\n",
            " 574.47783853 531.01846671 494.31233952 573.01113458 608.45347604\n",
            " 463.34174415 201.85466065 709.94626014 627.60461453 559.6939625\n",
            " 600.63424813 554.23327711 366.25551272 770.93985369 640.96045557\n",
            " 573.69632226 459.56450506 367.09114733 329.00392697 273.76424238]\n",
            "zbar_2017 [0.00082784 0.00592396 0.0162693  0.00831711 0.01149272 0.00364199\n",
            " 0.02794187 0.02603153 0.02649412 0.02588343 0.01898131 0.00098645\n",
            " 0.02398373 0.02837916 0.02750545 0.0280807  0.0275304  0.00982846\n",
            " 0.00437791 0.00834653 0.0212187  0.02077097 0.01650749 0.00157716\n",
            " 0.00345833]\n",
            "theta [0.0095654  0.00957145 0.14553405 1.22208617 1.43838269 0.41019622\n",
            " 0.60331954 1.12982865 1.68030923 1.95535797 1.73049776 1.03874073\n",
            " 1.47614774 1.48162579 2.1487447  1.88645796 1.92198288 2.11650298\n",
            " 2.22671783 2.38185139 2.9813128  2.60327426 2.11782433 2.70052198\n",
            " 2.66172557]\n",
            "thetaSD [0.0095654  0.00957145 0.14553405 1.22208617 1.43838269 0.41019622\n",
            " 0.60331954 1.12982865 1.68030923 1.95535797 1.73049776 1.03874073\n",
            " 1.47614774 1.48162579 2.1487447  1.88645796 1.92198288 2.11650298\n",
            " 2.22671783 2.38185139 2.9813128  2.60327426 2.11782433 2.70052198\n",
            " 2.66172557]\n"
          ]
        }
      ],
      "source": [
        "site_num = 25\n",
        "\n",
        "zbar_2017, gamma, gammaSD, z_2017, forestArea_2017_ha, theta, thetaSD = load_site_data(\n",
        "    site_num, norm_fac=1e9\n",
        ")\n",
        "print(\"gamma\", gamma)\n",
        "print(\"gammaSD\", gammaSD)\n",
        "print(\"z_2017\", z_2017)\n",
        "print(\"gamma - gammaSD\", gamma - gammaSD)\n",
        "print(\"zbar_2017\", zbar_2017)\n",
        "print(\"theta\", theta)\n",
        "print(\"thetaSD\", theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0bTyv_3X27a"
      },
      "source": [
        "## 2.2 Log Density Function of Posterior Density\n",
        "\n",
        "The function `log_density_function()` evaluates the log-density of the objective or posterior distribution. This function is used within an optimization loop, with some parameters updated in each iteration of the loop.\n",
        "To be specific, the function calculate the log of\n",
        "$$ \\exp\\left[ -\\frac 1 \\xi f(\\mathbf{d}, \\mathbf{\\beta})\\right]$$\n",
        "\n",
        "which is the numerator of equation [2] (part of the likelihood, the denominator of likelihood is constant, and we ignore it), times the prior density $\\pi(\\beta)$.\n",
        "\n",
        "The function takes a large number of parameters, some of which include:\n",
        "\n",
        "- `gamma_val` : Gamma values.\n",
        "- `gamma_vals_mean` : Mean of gamma values.\n",
        "- `theta_vals` : Theta values.\n",
        "- `site_precisions` : Precision of site data.\n",
        "- `alpha` : Alpha values.\n",
        "- `sol` : Solution values from optimization.\n",
        "- And several other parameters.\n",
        "\n",
        "Inside the function, the `gamma_val` is flattened and a few initial computations are made. `X_zero` is calculated using `gamma_val`, `forestArea_2017_ha`, and `norm_fac`. `X_dym` is calculated using `alpha_p_Adym`, `X_zero`, `Bdym`, and `omega`.\n",
        "\n",
        "The function then modifies the solution of `X` (i.e., `sol.value(X)`) for further computation. This modification includes shifting and scaling operations on the `X` matrix.\n",
        "\n",
        "The function computes the log-density in three steps:\n",
        "\n",
        "- `term_1` is calculated as the negative sum of the product of `ds_vect`, `sol.value(Ua)`, and `zeta`, all divided by 2.\n",
        "- `term_2` is computed as the sum of the product of `ds_vect`, `pf`, and the difference between successive elements of `X_dym`.\n",
        "- `term_3` is calculated as the sum of the product of `ds_vect` and the sum of `z_shifted_X`.\n",
        "\n",
        "The objective value `obj_val` is the sum of `term_1`, `term_2`, and `term_3`.\n",
        "\n",
        "The function also computes `norm_log_prob` as the negative half of the dot product of `gamma_val_dev` and the dot product of `site_precisions` and `gamma_val_dev`.\n",
        "\n",
        "The log density value `log_density_val` is calculated as `-1.0 / xi * obj_val + norm_log_prob`.\n",
        "\n",
        "If the global `_DEBUG` flag is set, the function prints out the values of `term_1`, `term_2`, `term_3`, `obj_val`, `norm_log_prob`, and `log_density_val`.\n",
        "\n",
        "Finally, the function returns the `log_density_val`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XD0luGsX2m0"
      },
      "outputs": [],
      "source": [
        "def log_density_function(\n",
        "    gamma_val,\n",
        "    gamma_vals_mean,\n",
        "    theta_vals,\n",
        "    site_precisions,\n",
        "    alpha,\n",
        "    sol,\n",
        "    X,\n",
        "    Ua,\n",
        "    Up,\n",
        "    zbar_2017,\n",
        "    forestArea_2017_ha,\n",
        "    norm_fac,\n",
        "    alpha_p_Adym,\n",
        "    Bdym,\n",
        "    leng,\n",
        "    T,\n",
        "    ds_vect,\n",
        "    zeta,\n",
        "    xi,\n",
        "    kappa,\n",
        "    pa,\n",
        "    pf,\n",
        "):\n",
        "    \"\"\"\n",
        "    Define a function to evaluate log-density of the objective/posterior distribution\n",
        "    Some of the input parameters are updated at each cycle of the outer loop (optimization loop),\n",
        "    and it becomes then easier/cheaper to udpate the function stamp and keep it separate here\n",
        "    \"\"\"\n",
        "    N = X.shape[1] - 1\n",
        "\n",
        "    gamma_val = np.asarray(gamma_val).flatten()\n",
        "    gamma_size = gamma_val.size\n",
        "    x0_vals = gamma_val.T.dot(forestArea_2017_ha) / norm_fac\n",
        "    X_zero = np.sum(x0_vals) * np.ones(leng)\n",
        "\n",
        "    # shifted_X = zbar_2017 - sol.value(X)[0:gamma_size, :-1]\n",
        "    shifted_X = sol.value(X)[0:gamma_size, :-1].copy()\n",
        "    for j in range(N):\n",
        "        shifted_X[:, j] = zbar_2017 - shifted_X[:, j]\n",
        "    omega = np.dot(gamma_val, alpha * shifted_X - sol.value(Up))\n",
        "\n",
        "    X_dym = np.zeros(T + 1)\n",
        "    X_dym[0] = np.sum(x0_vals)\n",
        "    X_dym[1:] = alpha_p_Adym * X_zero + np.dot(Bdym, omega.T)\n",
        "\n",
        "    z_shifted_X = sol.value(X)[0:gamma_size, :].copy()\n",
        "    scl = pa * theta_vals - pf * kappa\n",
        "    for j in range(N + 1):\n",
        "        z_shifted_X[:, j] *= scl\n",
        "\n",
        "    term_1 = -casadi.sum2(np.reshape(ds_vect[0:T], (1, T)) * sol.value(Ua) * zeta / 2)\n",
        "    term_2 = casadi.sum2(\n",
        "        np.reshape(ds_vect[0:T], (1, T)) * pf * (X_dym[1:] - X_dym[0:-1])\n",
        "    )  # X_dym is for time t+1\n",
        "    term_3 = casadi.sum2(\n",
        "        np.reshape(ds_vect, (1, N + 1)) * casadi.sum1(z_shifted_X)\n",
        "    )  # z_shifted_x is for time t\n",
        "\n",
        "    obj_val = term_1 + term_2 + term_3\n",
        "\n",
        "    gamma_val_dev = gamma_val - gamma_vals_mean\n",
        "    norm_log_prob = -0.5 * np.dot(gamma_val_dev, site_precisions.dot(gamma_val_dev))\n",
        "    log_density_val = -1.0 / xi * obj_val + norm_log_prob\n",
        "\n",
        "    if _DEBUG:\n",
        "        print(\"Term 1: \", term_1)\n",
        "        print(\"Term 2: \", term_2)\n",
        "        print(\"Term 3: \", term_3)\n",
        "        print(\"obj_val: \", obj_val)\n",
        "        print(\"norm_log_prob\", norm_log_prob)\n",
        "        print(\"log_density_val\", log_density_val)\n",
        "\n",
        "    return log_density_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4-AZzexX62d"
      },
      "source": [
        "# 3 Using Hamiltonian Monte Carlo to Approximate Posterior Distribution\n",
        "\n",
        "We seek to non-parametrically estimate the log of equation [2] excluding the constant term (denominator), using Hamiltonian Monte Carlo\n",
        "$$\n",
        "\\frac {\\exp\\left[ - {\\frac 1 \\xi } f({ d},  { \\beta} )\\right]}\n",
        "{\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f({ d}, \\tilde  { \\beta})\\right]d\\pi( \\tilde { \\beta} )} \\pi(\\beta)\n",
        "$$\n",
        "\n",
        "\n",
        "## 3.1 Overview of Algorithm\n",
        "The estimation begins by loading data related to the sites and performing some preliminary calculations on gamma values. It then calculates the mean and covariances from the site data. It sets up various data structures and matrices to hold information as it iterates through the main loop.\n",
        "\n",
        "The main loop is a convergence loop that continues until the maximum number of iterations is reached or the error is below a specified tolerance. In each loop, the function:\n",
        "\n",
        "- Updates the values of x0 and constructs matrices A and D using the current gamma values.\n",
        "\n",
        "- Defines the right-hand side of an equation to be solved by a solver.\n",
        "\n",
        "- Sets up and solves an optimization problem using the CasADi `Opti` class.\n",
        "\n",
        "- Generates samples using the Hamiltonian Monte Carlo method. It then updates the gamma values and computes an error metric for convergence.\n",
        "\n",
        "- If the convergence criterion is met, the function exits the loop, otherwise, it continues to the next iteration.\n",
        "\n",
        "The illustration concludes by sampling the final distribution densely and returning the results.\n",
        "\n",
        "## 3.2 Configuration and Settings\n",
        "- `norm_fac`, `delta_t`, `alpha`, `kappa`, `pf`, `pa`, `xi`, `zeta`: These seem to be configurations or settings for the model.\n",
        "\n",
        "- `max_iter`, `tol`, `T`, `N`: These are parameters that control the iterations in the optimization process.\n",
        "\n",
        "- `sample_size`, `mode_as_solution`, `final_sample_size`: These parameters relate to the sampling process for the Hamiltonian Monte Carlo method.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umMa_elux_tY"
      },
      "outputs": [],
      "source": [
        "# Configurations/Settings\n",
        "norm_fac = 1e9\n",
        "delta_t = 0.02\n",
        "alpha = 0.045007414\n",
        "kappa = 2.094215255\n",
        "pf = 20.76\n",
        "pa = 44.75\n",
        "xi = 0.01\n",
        "zeta = 1.66e-4 * 1e9  # zeta := 1.66e-4*norm_fac  #\n",
        "#\n",
        "max_iter = 200\n",
        "tol = 0.01  # convergence tolerance\n",
        "T = 200\n",
        "N = 200\n",
        "#\n",
        "sample_size = 1000  # simulations before convergence (to evaluate the mean)\n",
        "mode_as_solution = (\n",
        "    False  # If true, use the mode (point of high probability) as solution for gamma\n",
        ")\n",
        "final_sample_size = 100_00  # number of samples to collect after convergence\n",
        "\n",
        "# Evaluate Gamma values ()\n",
        "gamma_1_vals = gamma - gammaSD\n",
        "gamma_2_vals = gamma + gammaSD\n",
        "gamma_size = gamma.size\n",
        "\n",
        "# Evaluate mean and covariances from site data\n",
        "site_stdev = gammaSD\n",
        "site_covariances = np.diag(np.power(site_stdev, 2))\n",
        "site_precisions = np.linalg.inv(site_covariances)\n",
        "site_mean = gamma_1_vals / 2 + gamma_2_vals / 2\n",
        "\n",
        "# Retrieve z data for selected site(s)\n",
        "site_z_vals = z_2017\n",
        "\n",
        "# Initialize Gamma Values\n",
        "gamma_vals = gamma.copy()\n",
        "gamma_vals_mean = gamma.copy()\n",
        "gamma_vals_old = gamma.copy()\n",
        "\n",
        "# Theta Values\n",
        "theta_vals = theta\n",
        "\n",
        "# Householder to track sampled gamma values\n",
        "# gamma_vals_tracker       = np.empty((gamma_vals.size, sample_size+1))\n",
        "# gamma_vals_tracker[:, 0] = gamma_vals.copy()\n",
        "gamma_vals_tracker = [gamma_vals.copy()]\n",
        "\n",
        "# Collected Ensembles over all iterations; dictionary indexed by iteration number\n",
        "collected_ensembles = {}\n",
        "\n",
        "# Track error over iterations\n",
        "error_tracker = []\n",
        "\n",
        "# Update this parameter (leng) once figured out where it is coming from\n",
        "leng = 200\n",
        "arr = np.cumsum(\n",
        "    np.triu(np.ones((leng, leng))),\n",
        "    axis=1,\n",
        ").T\n",
        "Bdym = (1 - alpha) ** (arr - 1)\n",
        "Bdym[Bdym > 1] = 0.0\n",
        "Adym = np.arange(1, leng + 1)\n",
        "alpha_p_Adym = np.power(1 - alpha, Adym)\n",
        "\n",
        "# Initialize Blocks of the A matrix those won't change\n",
        "A = np.zeros((gamma_size + 2, gamma_size + 2))\n",
        "Ax = np.zeros(gamma_size + 2)\n",
        "\n",
        "# Construct Matrix B\n",
        "B = np.eye(N=gamma_size + 2, M=gamma_size, k=0)\n",
        "B = casadi.sparsify(B)\n",
        "\n",
        "# Construct Matrxi D constant blocks\n",
        "D = np.zeros((gamma_size + 2, gamma_size))\n",
        "\n",
        "# time step!\n",
        "dt = T / N\n",
        "\n",
        "# Other placeholders!\n",
        "ds_vect = np.exp(-delta_t * np.arange(N + 1) * dt)\n",
        "ds_vect = np.reshape(ds_vect, (ds_vect.size, 1))\n",
        "\n",
        "# Results dictionary\n",
        "results = dict(\n",
        "    gamma_size=gamma_size,\n",
        "    tol=tol,\n",
        "    T=T,\n",
        "    N=N,\n",
        "    norm_fac=norm_fac,\n",
        "    delta_t=delta_t,\n",
        "    alpha=alpha,\n",
        "    kappa=kappa,\n",
        "    pf=pf,\n",
        "    pa=pa,\n",
        "    xi=xi,\n",
        "    zeta=zeta,\n",
        "    sample_size=sample_size,\n",
        "    final_sample_size=final_sample_size,\n",
        "    mode_as_solution=mode_as_solution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LExjjGboX8_6"
      },
      "source": [
        "## 3.3 Hamiltonian Monte Carlo\n",
        "\n",
        "HMC stands for Hamiltonian Monte Carlo, a type of Markov Chain Monte Carlo (MCMC) method used in computational statistics to draw samples from a probability distribution, particularly high-dimensional ones.\n",
        "\n",
        "Below is a descirption of the HMC\n",
        "1. **Initialization**: Start with an initial guess for $\\beta$, typically denoted as $\\bar{\\beta}$. Simultaneously, generate a random value for $\\mu$ from its marginal distribution.\n",
        "\n",
        "2. **Symplectic Integration**: Use a discrete approximation method that preserves volume, also known as a symplectic integrator, to simulate the Hamiltonian dynamics. This process involves choosing a step size and running the simulation for a predetermined number of steps, typically around 20 but can be tailored based on the problem at hand. The output from this step is a new proposed point in the parameter space, denoted as $(\\beta^*,\\mu^*)$.\n",
        "\n",
        "3. **Metropolis Acceptance**: The Metropolis algorithm is then employed to decide whether to accept or reject the proposed point. The acceptance probability is given by $\\min\\{1, \\exp\\left(H(\\beta,\\mu)-H(\\beta^*,\\mu^*)\\right)\\}$, with discretization allowing for a probability less than 1.\n",
        "    - It is important to note that the volume-preservation property of the symplectic integrator guarantees that the Metropolis update leaves the canonical distribution for $(\\beta, \\mu)$ invariant.\n",
        "\n",
        "4. **Iteration**: Generate another random value for $\\mu'$ and repeat the steps 2 and 3 starting from the new point $(\\beta^*,\\mu')$. If the proposed point is rejected in the Metropolis step, we stick with the current $\\beta$ and repeat the process.\n",
        "\n",
        "5. **Decision Recalculation**: After each full iteration, the decision $d$ is recalculated.\n",
        "\n",
        "6. **Convergence**: The process from steps 2 to 5 is repeated until the average $\\beta$ value converges to a stable point.\n",
        "\n",
        "7. **Final Distribution**: Lastly, the HMC is run for a large number of iterations (e.g., 10,000) to produce the final distorted distribution. This comprehensive run allows for more robust estimation of the distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "F451bUfsX_sm",
        "outputId": "60f21137-d67a-497d-9616-423736d703ed"
      },
      "outputs": [],
      "source": [
        "# Initialize error & iteration counter\n",
        "error = np.infty\n",
        "cntr = 0\n",
        "\n",
        "# Loop until convergence\n",
        "while cntr < max_iter and error > tol:\n",
        "    # Update x0, initial distribution of carbon absorption of the amazon\n",
        "    x0_vals = gamma_vals * forestArea_2017_ha / norm_fac\n",
        "\n",
        "    # Construct Matrix A from new gamma_vals\n",
        "    A[:-2, :] = 0.0\n",
        "    Ax[0:gamma_size] = -alpha * gamma_vals[0:gamma_size]\n",
        "    Ax[-1] = alpha * np.sum(gamma_vals * zbar_2017)\n",
        "    Ax[-2] = -alpha\n",
        "    A[-2, :] = Ax\n",
        "    A[-1, :] = 0.0\n",
        "    A = casadi.sparsify(A)\n",
        "\n",
        "    # Construct Matrix D from new gamma_vals\n",
        "    D[:, :] = 0.0\n",
        "    D[-2, :] = -gamma_vals\n",
        "    D = casadi.sparsify(D)\n",
        "\n",
        "    # Define the right hand side (symbolic here) as a function of gamma\n",
        "    gamma = casadi.MX.sym(\"gamma\", gamma_size + 2)  # state\n",
        "    up = casadi.MX.sym(\"up\", gamma_size)  # control\n",
        "    um = casadi.MX.sym(\"um\", gamma_size)  # control\n",
        "\n",
        "    rhs = (A @ gamma + B @ (up - um) + D @ up) * dt + gamma\n",
        "    f = casadi.Function(\"f\", [gamma, um, up], [rhs])  # dynamics law of motion for state\n",
        "\n",
        "    ## Define an optimizer and initialize it, and set constraints\n",
        "    opti = casadi.Opti()\n",
        "\n",
        "    # Decision variables for states\n",
        "    X = opti.variable(gamma_size + 2, N + 1)\n",
        "\n",
        "    # Aliases for states\n",
        "    Up = opti.variable(gamma_size, N)\n",
        "    Um = opti.variable(gamma_size, N)\n",
        "    Ua = opti.variable(1, N)\n",
        "\n",
        "    # Parameter for initial state\n",
        "    ic = opti.parameter(gamma_size + 2)\n",
        "\n",
        "    # Gap-closing shooting constraints\n",
        "    for k in range(N):\n",
        "        opti.subject_to(X[:, k + 1] == f(X[:, k], Um[:, k], Up[:, k]))\n",
        "\n",
        "    # Initial and terminal constraints\n",
        "    opti.subject_to(X[:, 0] == ic)\n",
        "    opti.subject_to(opti.bounded(0, X[0:gamma_size, :], zbar_2017[0:gamma_size]))\n",
        "\n",
        "    # Objective: regularization of controls\n",
        "    for k in range(gamma_size):\n",
        "        opti.subject_to(opti.bounded(0, Um[k, :], casadi.inf))\n",
        "        opti.subject_to(opti.bounded(0, Up[k, :], casadi.inf))\n",
        "\n",
        "    opti.subject_to(Ua == casadi.sum1(Up + Um) ** 2)\n",
        "\n",
        "    # Set teh optimization problem\n",
        "    term1 = casadi.sum2(ds_vect[0:N, :].T * Ua * zeta / 2)\n",
        "    term2 = -casadi.sum2(ds_vect[0:N, :].T * (pf * (X[-2, 1:] - X[-2, 0:-1])))\n",
        "    term3 = -casadi.sum2(\n",
        "        ds_vect.T * casadi.sum1((pa * theta_vals - pf * kappa) * X[0:gamma_size, :])\n",
        "    )\n",
        "\n",
        "    opti.minimize(term1 + term2 + term3)\n",
        "\n",
        "    # Solve optimization problem\n",
        "    options = dict()\n",
        "    options[\"print_time\"] = False\n",
        "    options[\"expand\"] = True\n",
        "    options[\"ipopt\"] = {\n",
        "        \"print_level\": 0,\n",
        "        \"fast_step_computation\": \"yes\",\n",
        "        \"mu_allow_fast_monotone_decrease\": \"yes\",\n",
        "        \"warm_start_init_point\": \"yes\",\n",
        "    }\n",
        "    opti.solver(\"ipopt\", options)\n",
        "\n",
        "    opti.set_value(\n",
        "        ic,\n",
        "        casadi.vertcat(site_z_vals, np.sum(x0_vals), 1),\n",
        "    )\n",
        "\n",
        "    if _DEBUG:\n",
        "        print(\"ic: \", ic)\n",
        "        print(\"site_z_vals: \", site_z_vals)\n",
        "        print(\"x0_vals: \", x0_vals)\n",
        "        print(\n",
        "            \"casadi.vertcat(site_z_vals,np.sum(x0_vals),1): \",\n",
        "            casadi.vertcat(site_z_vals, np.sum(x0_vals), 1),\n",
        "        )\n",
        "    sol = opti.solve()\n",
        "\n",
        "    if _DEBUG:\n",
        "        print(\"sol.value(X)\", sol.value(X))\n",
        "        print(\"sol.value(Ua)\", sol.value(Ua))\n",
        "        print(\"sol.value(Up)\", sol.value(Up))\n",
        "        print(\"sol.value(Um)\", sol.value(Um))\n",
        "\n",
        "    ## Start Sampling\n",
        "    # Update signature of log density evaluator\n",
        "    log_density = lambda gamma_val: log_density_function(\n",
        "        gamma_val=gamma_val,\n",
        "        gamma_vals_mean=gamma_vals_mean,\n",
        "        theta_vals=theta_vals,\n",
        "        site_precisions=site_precisions,\n",
        "        alpha=alpha,\n",
        "        sol=sol,\n",
        "        X=X,\n",
        "        Ua=Ua,\n",
        "        Up=Up,\n",
        "        zbar_2017=zbar_2017,\n",
        "        forestArea_2017_ha=forestArea_2017_ha,\n",
        "        norm_fac=norm_fac,\n",
        "        alpha_p_Adym=alpha_p_Adym,\n",
        "        Bdym=Bdym,\n",
        "        leng=leng,\n",
        "        T=T,\n",
        "        ds_vect=ds_vect,\n",
        "        zeta=zeta,\n",
        "        xi=xi,\n",
        "        kappa=kappa,\n",
        "        pa=pa,\n",
        "        pf=pf,\n",
        "    )\n",
        "\n",
        "    # Create MCMC sampler & sample, then calculate diagnostics\n",
        "    sampler = create_hmc_sampler(\n",
        "        size=gamma_size,\n",
        "        log_density=log_density,\n",
        "        burn_in=100,\n",
        "        mix_in=2,\n",
        "        symplectic_integrator=\"verlet\",\n",
        "        symplectic_integrator_stepsize=1e-1,\n",
        "        symplectic_integrator_num_steps=3,\n",
        "        constraint_test=lambda x: True if np.all(x >= 0) else False,\n",
        "    )\n",
        "    gamma_post_samples = sampler.sample(\n",
        "        sample_size=sample_size,\n",
        "        initial_state=gamma_vals,\n",
        "        verbose=True,\n",
        "    )\n",
        "    gamma_post_samples = np.asarray(gamma_post_samples)\n",
        "\n",
        "    # Update ensemble/tracker\n",
        "    collected_ensembles.update({cntr: gamma_post_samples.copy()})\n",
        "\n",
        "    # Update gamma value\n",
        "    weight = 0.25  # <-- Not sure how this linear combination weighting helps!\n",
        "    if mode_as_solution:\n",
        "        raise NotImplementedError(\n",
        "            \"We will consider this in the future; trace sampled points and keep track of objective values to pick one with highest prob. \"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        gamma_vals = (\n",
        "            weight * np.mean(gamma_post_samples, axis=0) + (1 - weight) * gamma_vals_old\n",
        "        )\n",
        "    gamma_vals_tracker.append(gamma_vals.copy())\n",
        "\n",
        "    # Evaluate error for convergence check\n",
        "    error = np.max(np.abs(gamma_vals_old - gamma_vals) / gamma_vals_old)\n",
        "    error_tracker.append(error)\n",
        "    print(f\"Iteration [{cntr+1:4d}]: Error = {error}\")\n",
        "\n",
        "    # Exchange gamma values (for future weighting/update & error evaluation)\n",
        "    gamma_vals_old = gamma_vals\n",
        "\n",
        "    # Increase the counter\n",
        "    cntr += 1\n",
        "\n",
        "    results.update(\n",
        "        {\n",
        "            \"cntr\": cntr,\n",
        "            \"error_tracker\": np.asarray(error_tracker),\n",
        "            \"gamma_vals_tracker\": np.asarray(gamma_vals_tracker),\n",
        "            \"collected_ensembles\": collected_ensembles,\n",
        "        }\n",
        "    )\n",
        "    pickle.dump(results, open(\"results.pcl\", \"wb\"))\n",
        "\n",
        "    # Extensive plotting for monitoring; not needed really!\n",
        "    if False:\n",
        "        plt.plot(gamma_vals_tracker[-2], label=r\"Old $\\gamma$\")\n",
        "        plt.plot(gamma_vals_tracker[-1], label=r\"New $\\gamma$\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        for j in range(gamma_size):\n",
        "            plt.hist(gamma_post_samples[:, j], bins=50)\n",
        "            plt.title(f\"Iteration {cntr}; Site {j+1}\")\n",
        "            plt.show()\n",
        "\n",
        "print(\"Terminated. Sampling the final distribution\")\n",
        "# Sample (densly) the final distribution\n",
        "final_sample = sampler.sample(\n",
        "    sample_size=final_sample_size,\n",
        "    initial_state=gamma_vals,\n",
        "    verbose=True,\n",
        ")\n",
        "final_sample = np.asarray(final_sample)\n",
        "results.update({\"final_sample\": final_sample})\n",
        "pickle.dump(results, open(\"results.pcl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4P_aPkTYC50"
      },
      "source": [
        "## 3.4 Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgLSV--_YHI3"
      },
      "outputs": [],
      "source": [
        "# Plot Error Results\n",
        "plt.plot(results[\"error_tracker\"])\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-G5CN98YHt3"
      },
      "outputs": [],
      "source": [
        "# Plot Gamma Estimate Update\n",
        "for j in range(results[\"gamma_size\"]):\n",
        "    plt.plot(results[\"gamma_vals_tracker\"][:, j], label=r\"$\\gamma_{%d}$\" % (j + 1))\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6c4VH3kYJT6"
      },
      "outputs": [],
      "source": [
        "# Plot Gamma Estimate Update\n",
        "for j in range(results[\"gamma_size\"]):\n",
        "    plt.plot(results[\"gamma_vals_tracker\"][:, j], label=r\"$\\gamma_{%d}$\" % (j + 1))\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUVpwYgiYL50"
      },
      "outputs": [],
      "source": [
        "# Plot Histograms\n",
        "for itr in results[\"collected_ensembles\"].keys():\n",
        "    for j in range(results[\"gamma_size\"]):\n",
        "        plt.hist(results[\"collected_ensembles\"][itr][:, j], bins=100)\n",
        "        plt.title(f\"Iteration {itr+1}; Site {j+1}\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYV6IIGHYN8h"
      },
      "outputs": [],
      "source": [
        "# Plot Histogram of the final sample\n",
        "for j in range(results[\"gamma_size\"]):\n",
        "    plt.hist(results[\"final_sample\"][:, j], bins=100)\n",
        "    plt.title(f\"Final Sample; Site {j+1}\")\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
