{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Fvf20OgS8u"
      },
      "source": [
        "# Carbon prices and forest preservation over space and time in the Brazilian Amazon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3tc7UuOW4zp",
        "outputId": "5ef3008d-0495-48be-df23-fe287fc59e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing MCMC Module: mcmc_sampling_original.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import pickle\n",
        "import casadi\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "from mcmc.hmc import create_hmc_sampler\n",
        "\n",
        "# Local Debugging flag; remove when all tested\n",
        "_DEBUG = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkdacge-XQmn"
      },
      "source": [
        "# 1 Parameter Uncertainty\n",
        "\n",
        "# 1.1 Max-min problem\n",
        "We investigate a static formulation of robustness to parameter uncertainty.  For each site, we consider the parameter pair $\\beta^i = (\\gamma^i, \\theta^i)$ for $i=1,2,...,I$, where $θ_i$ is a site-specific productivity parameter, $γ^i \\geq 0$ denotes\n",
        "the density of CO2e that is present in a primary forest in site $i$. Let\n",
        "$\\mathbf{\\beta}$ denote full parameter vector including all sites and hence of dimension $2 \\times I.$\n",
        "\n",
        "Our planner is uncertain about these parameter values and instead has baseline\n",
        "probability distribution $\\pi.$  In addition, this planner is uncertain about what  distribution to use and instead  thinks of $\\pi$ as a rough approximation.  We address this uncertainty by\n",
        "introducing ambiguity about the parameter distribution.\n",
        "\n",
        "Let $d$ be the vector of decisions and $f( \\mathbf{d},\\mathbf{\\beta})$ for the resulting value  given the unknown parameter $\\mathbf{\\beta}.$  We use a divergence measure to capture  ambiguity about the parameter distribution.  For $\\int g( \\mathbf{\\beta}) d\\pi( \\mathbf{\\beta}) = 1,$ the relative entropy (or Kullback-Leibler) divergence\n",
        "\n",
        "$$\n",
        "\\int_{\\mathcal B} \\log g(\\mathbf{\\beta}) g( \\mathbf{\\beta}) d \\pi(\\mathbf{ \\beta}) \\ge 0,\n",
        "$$\n",
        "\n",
        "is a commonly used measure of divergence between a probability $g(\\beta)  d\\pi(  \\beta)$ and the baseline $d\\pi(\\beta).$\n",
        " To produce optimal controls that are robust to the parameter uncertainty, solve\n",
        "\n",
        "$$\n",
        "\\max_{ d} \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi( \\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g( \\mathbf{\\beta}) g(\\mathbf{ \\beta} ) d\\pi( \\mathbf{\\beta})\n",
        "$$\n",
        "\n",
        "where $\\xi > 0$ is penalty parameter. Alternatively, we can think of $\\xi$ a as Lagrange multiplier on a relative entropy divergence constraint.\n",
        "\n",
        "We implement a full commitment to the baseline distribution by  making $\\xi$  arbitrarily large.  More modest settings capture a concern for robustness.\n",
        "\n",
        "(WE SHOULD WRITE THE CODE SOLVE SEPERATELY THE MAX AND THE MIN PROBLEM)\n",
        "\n",
        "## 1.2 Worst case probability\n",
        "\n",
        "One nice feature of using relative entropy divergence to explore distributional sensitivity is that the minimization problem has a quasi-analytical solution:\n",
        "\n",
        "$$\n",
        "-\\xi \\log \\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{ d},  \\mathbf{ \\beta} )  \\right]  d\\pi( \\beta) = \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi(\\mathbf {\\beta})\n",
        "+ \\xi \\int \\log g(\\mathbf{ \\beta}) g( \\mathbf{\\beta}) d\\pi( \\mathbf{\\beta})\\tag{1}\n",
        "$$\n",
        "\n",
        "where the minimizing $g$ given by:\n",
        "\n",
        "$$\n",
        "g^* = \\frac {{\\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta} )\\right]}}\n",
        "{{\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d}, \\tilde  {\\mathbf{\\beta}})\\right]d\\pi( \\tilde {\\mathbf{\\beta}} )}}\\tag{2}\n",
        "$$\n",
        "\n",
        "This tilts the distribution towards smaller objectives for each given decision $\\mathbf{d}$. The candidate solution presumes that:\n",
        "\n",
        "$$\n",
        "\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta})\\right]d\\pi(\\mathbf{\\beta} ) < \\infty\n",
        "$$\n",
        "\n",
        "which implicitly limits how fat the tail can be for the distribution $\\pi$.\n",
        "\n",
        "For conceptual reasons, we also switch the order of the maximization and minimization.  Under quite general conditions, we can invoke the min-max theorem:\n",
        "\n",
        "$$\n",
        "\\min_{g, \\int g d\\pi = 1} \\max_{\\mathbf{d}}  \\int_{\\mathcal B} f(\\mathbf{d},  \\mathbf{\\beta} )   g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g(\\mathbf{\\beta}) g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta})\n",
        "$$\n",
        "where $\\xi > 0$ is penalty parameter.\n",
        "\n",
        "Consider the inner maximization problem:\n",
        "\n",
        "$$\n",
        "\\max_{\\mathbf{d}}  \\int_{\\mathcal B} f(\\mathbf{d},  \\mathbf{\\beta} )   g(\\mathbf{\\beta}) d\\pi(\\mathbf{\\beta})\n",
        "$$\n",
        "\n",
        "where we are free to drop the relative entropy penalty as it does not depend on the decision $\\mathbf{d}$.  Provided that this problem has a solution for the outer $g$ minimization, then the planner is maximizing against this particular (penalized) \"worst case probability.\"\n",
        "\n",
        "This computation is of interest as a way to interpret the consequences of any given choice of the penalty parameter $\\xi$.  In practice, we find it revealing to explore alternative choices of $\\xi$ and deduce their implications for the implied worst case probabilities. This follows a common practice for robust Bayesian methods.\n",
        "\n",
        "## 1.3 Numerical Solution\n",
        "For solving the robust problem numerically, we take an iterative approach, also supported by the min-max theorem.\n",
        "\n",
        "Specifically, we proceed as follows:\n",
        "\n",
        "1. Given a $g$, we solve the maximization problem for a candidate $\\mathbf{d}.$  We ignore the relative entropy penalty term in this solution.\n",
        "\n",
        "$$\n",
        "\\max_{ d}\\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi( \\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g( \\mathbf{\\beta}) g(\\mathbf{ \\beta} ) d\\pi( \\mathbf{\\beta}),\\text{ given }g\n",
        "$$\n",
        "\n",
        "2. For a given $\\mathbf{d}$, we solve the minimization problem with the relative entropy term to obtain a new candidate for $g$.\n",
        "\n",
        "$$\n",
        " \\min_{g, \\int g d\\pi = 1} \\int_{\\mathcal B} f(\\mathbf{ d},  \\mathbf{ \\beta} )   g(\\mathbf{ \\beta}) d\\pi( \\mathbf{\\beta})\n",
        "+ \\xi \\int \\log g( \\mathbf{\\beta}) g(\\mathbf{ \\beta} ) d\\pi( \\mathbf{\\beta}), \\text{ given }\\mathbf{ d}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "g^* = \\frac {{\\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d},  \\mathbf{\\beta} )\\right]}}\n",
        "{{\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f(\\mathbf{d}, \\tilde  {\\mathbf{\\beta}})\\right]d\\pi( \\tilde {\\mathbf{\\beta}} )}}\n",
        "$$\n",
        "3. We repeat the steps until we achieve convergence.\n",
        "\n",
        "For step 2, we use a Hamiltonian simulation approach along with the quasi-analytical solution in computing equation [2].  A numerical method is necessary because of the denominator term, and Markov simulation gives us one way to explore a parameter space of potentially large dimension.  Very similar to how a Metropolis-Hastings algorithm can be used  to compute a Bayesian posterior in terms of a prior and a likelihood, we calculate the exponentially tilted solution using $d\\pi(\\mathbf{\\beta})$ and $\\exp\\left[-{\\frac 1 \\xi} f(\\mathbf{d}, \\mathbf{\\beta})\\right].$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7qTpY1y4KDT"
      },
      "source": [
        "# 2 Optimization Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OtkIk3XXZFk"
      },
      "source": [
        "## 2.1 Load Site Data Function\n",
        "\n",
        "The function `load_site_data(site_num, norm_fac=1.0)` is designed to load and preprocess site-specific data, including the prior for $\\beta^i = (\\gamma^i, \\theta^i)$. The function takes two parameters:\n",
        "\n",
        "- `site_num` : An integer representing the site number.\n",
        "- `norm_fac` : A float representing the normalization factor. Its default value is 1.0.\n",
        "\n",
        "The function starts by reading a CSV file specific to the site number passed as the `site_num` parameter. The file should be named as \"calibration_{n}SitesModel.csv\", where `{n}` is the site number.\n",
        "\n",
        "The function then extracts information from several columns in the data frame. The columns are named using the site number and the corresponding information type (e.g., `z_2017_{n}Sites`, `zbar_2017_{n}Sites`, etc.).\n",
        "\n",
        "The extracted information includes:\n",
        "\n",
        "- `z_2017` : Data specific to the site from 2017.\n",
        "- `zbar_2017` : Mean data specific to the site from 2017.\n",
        "- `gamma` : Gamma value for the site.\n",
        "- `gammaSD` : Standard deviation of the gamma value for the site.\n",
        "- `forestArea_2017_ha` : Forest area for the site in 2017 (in hectares).\n",
        "- `theta` : Theta value for the site.\n",
        "\n",
        "After extracting the data, the function normalizes `zbar_2017` and `z_2017` using the normalization factor `norm_fac`.\n",
        "\n",
        "Finally, the function returns a tuple containing the normalized `zbar_2017`, `gamma`, `gammaSD`, `z_2017`, `forestArea_2017_ha`, and `theta` data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EirRY4AXXWe",
        "outputId": "90676f36-95e0-4c22-c458-666b83816593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gamma [651.4416252 477.8084078 716.9695451 533.3620973 545.2476866 663.1570507\n",
            " 504.8877277 785.4266392 382.0968552 359.7794996 255.0235625 578.6855671]\n",
            "gammaSD [279.2824616 189.5096353 307.5545054 247.0644443 238.5838156 294.0275391\n",
            " 241.5341456 331.0946171 180.9862332 174.8798082 133.5485458 226.3262296]\n",
            "z_2017 [2.45444396e-03 7.63690458e-04 1.34062943e-05 8.00264438e-03\n",
            " 2.48574718e-03 5.95173995e-03 7.14563902e-03 5.39293148e-04\n",
            " 1.59867043e-02 8.38457577e-03 5.34956545e-03 2.01880643e-04]\n",
            "gamma - gammaSD [372.1591636 288.2987725 409.4150397 286.297653  306.663871  369.1295116\n",
            " 263.3535821 454.3320221 201.110622  184.8996914 121.4750167 352.3593375]\n",
            "zbar_2017 [0.06706991 0.02872241 0.01190116 0.03584147 0.04888781 0.04683603\n",
            " 0.01892768 0.02768885 0.03492384 0.01736616 0.00937561 0.04681555]\n",
            "theta [1.16755587 0.20137289 0.18455511 1.81783429 0.92553791 2.04819135\n",
            " 2.77647057 1.83848898 1.91426269 3.23500525 1.93165613 0.04961549]\n"
          ]
        }
      ],
      "source": [
        "site_num = 12\n",
        "\n",
        "\n",
        "def load_site_data(\n",
        "    site_num,\n",
        "    norm_fac=1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Load site data\n",
        "\n",
        "    :returns:\n",
        "        -\n",
        "    \"\"\"\n",
        "    # Read data file\n",
        "    n = site_num\n",
        "    df = pd.read_csv(f\"../data/calibration_{n}SitesModel.csv\")\n",
        "\n",
        "    # Extract information\n",
        "    z_2017 = df[f\"z_2017_{n}Sites\"].to_numpy()\n",
        "    zbar_2017 = df[f\"zbar_2017_{n}Sites\"].to_numpy()\n",
        "    gamma = df[f\"gamma_{n}Sites\"].to_numpy()\n",
        "    gammaSD = df[f\"gammaSD_{n}Sites\"].to_numpy()\n",
        "    forestArea_2017_ha = df[f\"forestArea_2017_ha_{n}Sites\"].to_numpy()\n",
        "    theta = df[f\"theta_{n}Sites\"].to_numpy()\n",
        "\n",
        "    # Normalize Z data\n",
        "    zbar_2017 /= norm_fac\n",
        "    z_2017 /= norm_fac\n",
        "\n",
        "    return (zbar_2017, gamma, gammaSD, z_2017, forestArea_2017_ha, theta)\n",
        "\n",
        "\n",
        "zbar_2017, gamma, gammaSD, z_2017, forestArea_2017_ha, theta = load_site_data(\n",
        "    site_num, norm_fac=1e9\n",
        ")\n",
        "print(\"gamma\", gamma)\n",
        "print(\"gammaSD\", gammaSD)\n",
        "print(\"z_2017\", z_2017)\n",
        "print(\"gamma - gammaSD\", gamma - gammaSD)\n",
        "print(\"zbar_2017\", zbar_2017)\n",
        "print(\"theta\", theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0bTyv_3X27a"
      },
      "source": [
        "## 2.2 Log Density Function of Posterior Density\n",
        "\n",
        "The function `log_density_function()` evaluates the log-density of the objective or posterior distribution. This function is used within an optimization loop, with some parameters updated in each iteration of the loop.\n",
        "To be specific, the function calculate the log of\n",
        "$$ \\exp\\left[ -\\frac 1 \\xi f(\\mathbf{d}, \\mathbf{\\beta})\\right]$$\n",
        "\n",
        "which is the numerator of equation [2] (part of the likelihood, the denominator of likelihood is constant, and we ignore it), times the prior density $\\pi(\\beta)$.\n",
        "\n",
        "The function takes a large number of parameters, some of which include:\n",
        "\n",
        "- `gamma_val` : Gamma values.\n",
        "- `gamma_vals_mean` : Mean of gamma values.\n",
        "- `theta_vals` : Theta values.\n",
        "- `site_precisions` : Precision of site data.\n",
        "- `alpha` : Alpha values.\n",
        "- `sol` : Solution values from optimization.\n",
        "- And several other parameters.\n",
        "\n",
        "Inside the function, the `gamma_val` is flattened and a few initial computations are made. `X_zero` is calculated using `gamma_val`, `forestArea_2017_ha`, and `norm_fac`. `X_dym` is calculated using `alpha_p_Adym`, `X_zero`, `Bdym`, and `omega`.\n",
        "\n",
        "The function then modifies the solution of `X` (i.e., `sol.value(X)`) for further computation. This modification includes shifting and scaling operations on the `X` matrix.\n",
        "\n",
        "The function computes the log-density in three steps:\n",
        "\n",
        "- `term_1` is calculated as the negative sum of the product of `ds_vect`, `sol.value(Ua)`, and `zeta`, all divided by 2.\n",
        "- `term_2` is computed as the sum of the product of `ds_vect`, `pf`, and the difference between successive elements of `X_dym`.\n",
        "- `term_3` is calculated as the sum of the product of `ds_vect` and the sum of `z_shifted_X`.\n",
        "\n",
        "The objective value `obj_val` is the sum of `term_1`, `term_2`, and `term_3`.\n",
        "\n",
        "The function also computes `norm_log_prob` as the negative half of the dot product of `gamma_val_dev` and the dot product of `site_precisions` and `gamma_val_dev`.\n",
        "\n",
        "The log density value `log_density_val` is calculated as `-1.0 / xi * obj_val + norm_log_prob`.\n",
        "\n",
        "If the global `_DEBUG` flag is set, the function prints out the values of `term_1`, `term_2`, `term_3`, `obj_val`, `norm_log_prob`, and `log_density_val`.\n",
        "\n",
        "Finally, the function returns the `log_density_val`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7XD0luGsX2m0"
      },
      "outputs": [],
      "source": [
        "def log_density_function(\n",
        "    gamma_val,\n",
        "    gamma_vals_mean,\n",
        "    theta_vals,\n",
        "    site_precisions,\n",
        "    alpha,\n",
        "    sol,\n",
        "    X,\n",
        "    Ua,\n",
        "    Up,\n",
        "    zbar_2017,\n",
        "    forestArea_2017_ha,\n",
        "    norm_fac,\n",
        "    alpha_p_Adym,\n",
        "    Bdym,\n",
        "    leng,\n",
        "    T,\n",
        "    ds_vect,\n",
        "    zeta,\n",
        "    xi,\n",
        "    kappa,\n",
        "    pa,\n",
        "    pf,\n",
        "):\n",
        "    \"\"\"\n",
        "    Define a function to evaluate log-density of the objective/posterior distribution\n",
        "    Some of the input parameters are updated at each cycle of the outer loop (optimization loop),\n",
        "    and it becomes then easier/cheaper to udpate the function stamp and keep it separate here\n",
        "    \"\"\"\n",
        "    N = X.shape[1] - 1\n",
        "\n",
        "    gamma_val = np.asarray(gamma_val).flatten()\n",
        "    gamma_size = gamma_val.size\n",
        "    x0_vals = gamma_val.T.dot(forestArea_2017_ha) / norm_fac\n",
        "    X_zero = np.sum(x0_vals) * np.ones(leng)\n",
        "\n",
        "    # shifted_X = zbar_2017 - sol.value(X)[0:gamma_size, :-1]\n",
        "    shifted_X = sol.value(X)[0:gamma_size, :-1].copy()\n",
        "    for j in range(N):\n",
        "        shifted_X[:, j] = zbar_2017 - shifted_X[:, j]\n",
        "    omega = np.dot(gamma_val, alpha * shifted_X - sol.value(Up))\n",
        "\n",
        "    X_dym = np.zeros(T + 1)\n",
        "    X_dym[0] = np.sum(x0_vals)\n",
        "    X_dym[1:] = alpha_p_Adym * X_zero + np.dot(Bdym, omega.T)\n",
        "\n",
        "    z_shifted_X = sol.value(X)[0:gamma_size, :].copy()\n",
        "    scl = pa * theta_vals - pf * kappa\n",
        "    for j in range(N + 1):\n",
        "        z_shifted_X[:, j] *= scl\n",
        "\n",
        "    term_1 = -casadi.sum2(np.reshape(ds_vect[0:T], (1, T)) * sol.value(Ua) * zeta / 2)\n",
        "    term_2 = casadi.sum2(\n",
        "        np.reshape(ds_vect[0:T], (1, T)) * pf * (X_dym[1:] - X_dym[0:-1])\n",
        "    )  # X_dym is for time t+1\n",
        "    term_3 = casadi.sum2(\n",
        "        np.reshape(ds_vect, (1, N + 1)) * casadi.sum1(z_shifted_X)\n",
        "    )  # z_shifted_x is for time t\n",
        "\n",
        "    obj_val = term_1 + term_2 + term_3\n",
        "\n",
        "    gamma_val_dev = gamma_val - gamma_vals_mean\n",
        "    norm_log_prob = -0.5 * np.dot(gamma_val_dev, site_precisions.dot(gamma_val_dev))\n",
        "    log_density_val = -1.0 / xi * obj_val + norm_log_prob\n",
        "\n",
        "    if _DEBUG:\n",
        "        print(\"Term 1: \", term_1)\n",
        "        print(\"Term 2: \", term_2)\n",
        "        print(\"Term 3: \", term_3)\n",
        "        print(\"obj_val: \", obj_val)\n",
        "        print(\"norm_log_prob\", norm_log_prob)\n",
        "        print(\"log_density_val\", log_density_val)\n",
        "\n",
        "    return log_density_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4-AZzexX62d"
      },
      "source": [
        "# 3 Using Hamiltonian Monte Carlo to Approximate Posterior Distribution\n",
        "\n",
        "We seek to non-parametrically estimate the log of equation [2] excluding the constant term (denominator), using Hamiltonian Monte Carlo\n",
        "$$\n",
        "\\frac {\\exp\\left[ - {\\frac 1 \\xi } f({ d},  { \\beta} )\\right]}\n",
        "{\\int_{\\mathcal B} \\exp\\left[ - {\\frac 1 \\xi } f({ d}, \\tilde  { \\beta})\\right]d\\pi( \\tilde { \\beta} )} \\pi(\\beta)\n",
        "$$\n",
        "\n",
        "\n",
        "## 3.1 Overview of Algorithm\n",
        "The estimation begins by loading data related to the sites and performing some preliminary calculations on gamma values. It then calculates the mean and covariances from the site data. It sets up various data structures and matrices to hold information as it iterates through the main loop.\n",
        "\n",
        "The main loop is a convergence loop that continues until the maximum number of iterations is reached or the error is below a specified tolerance. In each loop, the function:\n",
        "\n",
        "- Updates the values of x0 and constructs matrices A and D using the current gamma values.\n",
        "\n",
        "- Defines the right-hand side of an equation to be solved by a solver.\n",
        "\n",
        "- Sets up and solves an optimization problem using the CasADi `Opti` class.\n",
        "\n",
        "- Generates samples using the Hamiltonian Monte Carlo method. It then updates the gamma values and computes an error metric for convergence.\n",
        "\n",
        "- If the convergence criterion is met, the function exits the loop, otherwise, it continues to the next iteration.\n",
        "\n",
        "The illustration concludes by sampling the final distribution densely and returning the results.\n",
        "\n",
        "## 3.2 Configuration and Settings\n",
        "- `norm_fac`, `delta_t`, `alpha`, `kappa`, `pf`, `pa`, `xi`, `zeta`: These seem to be configurations or settings for the model.\n",
        "\n",
        "- `max_iter`, `tol`, `T`, `N`: These are parameters that control the iterations in the optimization process.\n",
        "\n",
        "- `sample_size`, `mode_as_solution`, `final_sample_size`: These parameters relate to the sampling process for the Hamiltonian Monte Carlo method.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "umMa_elux_tY"
      },
      "outputs": [],
      "source": [
        "# Configurations/Settings\n",
        "norm_fac = 1e9\n",
        "delta_t = 0.02\n",
        "alpha = 0.045007414\n",
        "kappa = 2.094215255\n",
        "pf = 20.76\n",
        "pa = 44.75\n",
        "xi = 0.01\n",
        "zeta = 1.66e-4 * 1e9  # zeta := 1.66e-4*norm_fac  #\n",
        "#\n",
        "max_iter = 200\n",
        "tol = 0.01  # convergence tolerance\n",
        "T = 200\n",
        "N = 200\n",
        "#\n",
        "sample_size = 1000  # simulations before convergence (to evaluate the mean)\n",
        "mode_as_solution = (\n",
        "    False  # If true, use the mode (point of high probability) as solution for gamma\n",
        ")\n",
        "final_sample_size = 100_00  # number of samples to collect after convergence\n",
        "\n",
        "# Evaluate Gamma values ()\n",
        "gamma_1_vals = gamma - gammaSD\n",
        "gamma_2_vals = gamma + gammaSD\n",
        "gamma_size = gamma.size\n",
        "\n",
        "# Evaluate mean and covariances from site data\n",
        "site_stdev = gammaSD\n",
        "site_covariances = np.diag(np.power(site_stdev, 2))\n",
        "site_precisions = np.linalg.inv(site_covariances)\n",
        "site_mean = gamma_1_vals / 2 + gamma_2_vals / 2\n",
        "\n",
        "# Retrieve z data for selected site(s)\n",
        "site_z_vals = z_2017\n",
        "\n",
        "# Initialize Gamma Values\n",
        "gamma_vals = gamma.copy()\n",
        "gamma_vals_mean = gamma.copy()\n",
        "gamma_vals_old = gamma.copy()\n",
        "\n",
        "# Theta Values\n",
        "theta_vals = theta\n",
        "\n",
        "# Householder to track sampled gamma values\n",
        "# gamma_vals_tracker       = np.empty((gamma_vals.size, sample_size+1))\n",
        "# gamma_vals_tracker[:, 0] = gamma_vals.copy()\n",
        "gamma_vals_tracker = [gamma_vals.copy()]\n",
        "\n",
        "# Collected Ensembles over all iterations; dictionary indexed by iteration number\n",
        "collected_ensembles = {}\n",
        "\n",
        "# Track error over iterations\n",
        "error_tracker = []\n",
        "\n",
        "# Update this parameter (leng) once figured out where it is coming from\n",
        "leng = 200\n",
        "arr = np.cumsum(\n",
        "    np.triu(np.ones((leng, leng))),\n",
        "    axis=1,\n",
        ").T\n",
        "Bdym = (1 - alpha) ** (arr - 1)\n",
        "Bdym[Bdym > 1] = 0.0\n",
        "Adym = np.arange(1, leng + 1)\n",
        "alpha_p_Adym = np.power(1 - alpha, Adym)\n",
        "\n",
        "# Initialize Blocks of the A matrix those won't change\n",
        "A = np.zeros((gamma_size + 2, gamma_size + 2))\n",
        "Ax = np.zeros(gamma_size + 2)\n",
        "\n",
        "# Construct Matrix B\n",
        "B = np.eye(N=gamma_size + 2, M=gamma_size, k=0)\n",
        "B = casadi.sparsify(B)\n",
        "\n",
        "# Construct Matrxi D constant blocks\n",
        "D = np.zeros((gamma_size + 2, gamma_size))\n",
        "\n",
        "# time step!\n",
        "dt = T / N\n",
        "\n",
        "# Other placeholders!\n",
        "ds_vect = np.exp(-delta_t * np.arange(N + 1) * dt)\n",
        "ds_vect = np.reshape(ds_vect, (ds_vect.size, 1))\n",
        "\n",
        "# Results dictionary\n",
        "results = dict(\n",
        "    gamma_size=gamma_size,\n",
        "    tol=tol,\n",
        "    T=T,\n",
        "    N=N,\n",
        "    norm_fac=norm_fac,\n",
        "    delta_t=delta_t,\n",
        "    alpha=alpha,\n",
        "    kappa=kappa,\n",
        "    pf=pf,\n",
        "    pa=pa,\n",
        "    xi=xi,\n",
        "    zeta=zeta,\n",
        "    sample_size=sample_size,\n",
        "    final_sample_size=final_sample_size,\n",
        "    mode_as_solution=mode_as_solution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LExjjGboX8_6"
      },
      "source": [
        "## 3.3 Hamiltonian Monte Carlo\n",
        "\n",
        "HMC stands for Hamiltonian Monte Carlo, a type of Markov Chain Monte Carlo (MCMC) method used in computational statistics to draw samples from a probability distribution, particularly high-dimensional ones.\n",
        "\n",
        "Below is a descirption of the HMC\n",
        "1. **Initialization**: Start with an initial guess for $\\beta$, typically denoted as $\\bar{\\beta}$. Simultaneously, generate a random value for $\\mu$ from its marginal distribution.\n",
        "\n",
        "2. **Symplectic Integration**: Use a discrete approximation method that preserves volume, also known as a symplectic integrator, to simulate the Hamiltonian dynamics. This process involves choosing a step size and running the simulation for a predetermined number of steps, typically around 20 but can be tailored based on the problem at hand. The output from this step is a new proposed point in the parameter space, denoted as $(\\beta^*,\\mu^*)$.\n",
        "\n",
        "3. **Metropolis Acceptance**: The Metropolis algorithm is then employed to decide whether to accept or reject the proposed point. The acceptance probability is given by $\\min\\{1, \\exp\\left(H(\\beta,\\mu)-H(\\beta^*,\\mu^*)\\right)\\}$, with discretization allowing for a probability less than 1.\n",
        "    - It is important to note that the volume-preservation property of the symplectic integrator guarantees that the Metropolis update leaves the canonical distribution for $(\\beta, \\mu)$ invariant.\n",
        "\n",
        "4. **Iteration**: Generate another random value for $\\mu'$ and repeat the steps 2 and 3 starting from the new point $(\\beta^*,\\mu')$. If the proposed point is rejected in the Metropolis step, we stick with the current $\\beta$ and repeat the process.\n",
        "\n",
        "5. **Decision Recalculation**: After each full iteration, the decision $d$ is recalculated.\n",
        "\n",
        "6. **Convergence**: The process from steps 2 to 5 is repeated until the average $\\beta$ value converges to a stable point.\n",
        "\n",
        "7. **Final Distribution**: Lastly, the HMC is run for a large number of iterations (e.g., 10,000) to produce the final distorted distribution. This comprehensive run allows for more robust estimation of the distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "F451bUfsX_sm",
        "outputId": "60f21137-d67a-497d-9616-423736d703ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ic:  opti0_p_1\n",
            "site_z_vals:  [2.45444396e-03 7.63690458e-04 1.34062943e-05 8.00264438e-03\n",
            " 2.48574718e-03 5.95173995e-03 7.14563902e-03 5.39293148e-04\n",
            " 1.59867043e-02 8.38457577e-03 5.34956545e-03 2.01880643e-04]\n",
            "x0_vals:  [42.09320344 13.35891209  8.52315973 14.8481723  25.30061766 27.11270216\n",
            "  5.94860755 21.32398142  7.23581843  3.23138962  1.02673523 26.97465876]\n",
            "casadi.vertcat(site_z_vals,np.sum(x0_vals),1):  [0.00245444, 0.00076369, 1.34063e-05, 0.00800264, 0.00248575, 0.00595174, 0.00714564, 0.000539293, 0.0159867, 0.00838458, 0.00534957, 0.000201881, 196.978, 1]\n",
            "\n",
            "******************************************************************************\n",
            "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
            " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
            "         For more information visit https://github.com/coin-or/Ipopt\n",
            "******************************************************************************\n",
            "\n",
            "sol.value(X) [[2.45444396e-03 9.67907310e-08 5.06780592e-08 ... 2.93277317e-08\n",
            "  2.03562624e-08 7.02029752e-09]\n",
            " [7.63690458e-04 1.30162275e-07 6.11844011e-08 ... 3.98233011e-08\n",
            "  2.85723298e-08 1.15849202e-08]\n",
            " [1.34062942e-05 2.77174219e-08 1.62616815e-08 ... 2.38319201e-08\n",
            "  1.61717673e-08 4.65256365e-09]\n",
            " ...\n",
            " [2.01880643e-04 4.63071188e-08 2.68188067e-08 ... 3.11655457e-08\n",
            "  2.18687868e-08 7.83405552e-09]\n",
            " [1.96977958e+02 1.96978022e+02 1.97152205e+02 ... 2.23154275e+02\n",
            "  2.23154530e+02 2.23154780e+02]\n",
            " [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00]]\n",
            "sol.value(Ua) [ 4.16984946e-05  3.19705304e-05  2.40333307e-05  1.84858819e-05\n",
            "  1.57945733e-05  1.32670004e-05  1.09153341e-05  8.75239242e-06\n",
            "  6.79165394e-06  5.12980479e-06  3.67429202e-06  2.43906863e-06\n",
            "  1.69322974e-06  1.30339629e-06  9.58172912e-07  6.60602894e-07\n",
            "  4.13878932e-07  2.21351312e-07  1.79717955e-07  1.72486208e-07\n",
            "  1.65260953e-07  1.58048569e-07  1.50855820e-07  1.43689873e-07\n",
            "  1.36558322e-07  1.29469206e-07  1.22431029e-07  1.15452785e-07\n",
            "  1.08543977e-07  1.01714646e-07  9.49753948e-08  8.83374130e-08\n",
            "  8.18125067e-08  7.54131269e-08  6.91524005e-08  6.30441622e-08\n",
            "  5.71029886e-08  5.13442337e-08  4.57840672e-08  4.04395146e-08\n",
            "  3.53285013e-08  3.04699001e-08  2.58835844e-08  2.15904872e-08\n",
            "  1.76126700e-08  1.39733986e-08  1.06972228e-08  7.81002649e-09\n",
            "  5.33894828e-09  3.31188908e-09  1.75606993e-09  6.95851418e-10\n",
            "  1.37701634e-10 -2.40842036e-11 -2.69762775e-11 -1.26397061e-11\n",
            " -5.26427852e-12 -2.28423838e-12 -1.08629789e-12 -5.72947235e-13\n",
            " -3.33401863e-13 -2.11359356e-13 -1.43845112e-13 -1.03653274e-13\n",
            " -7.81637432e-14 -6.11121099e-14 -4.91889107e-14 -4.05445840e-14\n",
            " -3.40921545e-14 -2.91636003e-14 -2.53317586e-14 -2.23136998e-14\n",
            " -1.99160838e-14 -1.80030790e-14 -1.64768778e-14 -1.52654948e-14\n",
            " -1.43149098e-14 -1.35838794e-14 -1.30404296e-14 -1.26594354e-14\n",
            " -1.24209179e-14 -1.23088236e-14 -1.23101368e-14 -1.24142227e-14\n",
            " -1.26123353e-14 -1.28972444e-14 -1.32629486e-14 -1.37044523e-14\n",
            " -1.42175913e-14 -1.47988939e-14 -1.54454709e-14 -1.61549258e-14\n",
            " -1.69252832e-14 -1.77549285e-14 -1.86425604e-14 -1.95871493e-14\n",
            " -2.05879044e-14 -2.16442447e-14 -2.27557752e-14 -2.39222669e-14\n",
            " -2.51436386e-14 -2.64199422e-14 -2.77513494e-14 -2.91381404e-14\n",
            " -3.05806935e-14 -3.20794756e-14 -3.36350346e-14 -3.52479912e-14\n",
            " -3.69190319e-14 -3.86489027e-14 -4.04384019e-14 -4.22883744e-14\n",
            " -4.41997051e-14 -4.61733129e-14 -4.82101440e-14 -5.03111653e-14\n",
            " -5.24773576e-14 -5.47097082e-14 -5.70092031e-14 -5.93768187e-14\n",
            " -6.18135129e-14 -6.43202154e-14 -6.68978172e-14 -6.95471595e-14\n",
            " -7.22690214e-14 -7.50641064e-14 -7.79330284e-14 -8.08762957e-14\n",
            " -8.38942944e-14 -8.69872697e-14 -9.01553069e-14 -9.33983095e-14\n",
            " -9.67159767e-14 -1.00107779e-13 -1.03572932e-13 -1.07110369e-13\n",
            " -1.10718712e-13 -1.14396237e-13 -1.18140848e-13 -1.21950038e-13\n",
            " -1.25820853e-13 -1.29749860e-13 -1.33733104e-13 -1.37766075e-13\n",
            " -1.41843667e-13 -1.45960140e-13 -1.50109083e-13 -1.54283381e-13\n",
            " -1.58475180e-13 -1.62675854e-13 -1.66875986e-13 -1.71065341e-13\n",
            " -1.75232855e-13 -1.79366627e-13 -1.83453925e-13 -1.87481194e-13\n",
            " -1.91434086e-13 -1.95297497e-13 -1.99055619e-13 -2.02692015e-13\n",
            " -2.06189705e-13 -2.09531271e-13 -2.12698990e-13 -2.15674975e-13\n",
            " -2.18441348e-13 -2.20980425e-13 -2.23274927e-13 -2.25308205e-13\n",
            " -2.27064484e-13 -2.28529120e-13 -2.29688867e-13 -2.30532161e-13\n",
            " -2.31049396e-13 -2.31233211e-13 -2.31078777e-13 -2.30584072e-13\n",
            " -2.29750156e-13 -2.28581449e-13 -2.27085995e-13 -2.25275744e-13\n",
            " -2.23166852e-13 -2.20780017e-13 -2.18140895e-13 -2.15280636e-13\n",
            " -2.12236612e-13 -2.09053456e-13 -2.05784581e-13 -2.02494447e-13\n",
            " -1.99262043e-13 -1.96186318e-13 -1.93394933e-13 -1.91058819e-13\n",
            " -1.89417449e-13 -1.88825168e-13 -1.89842222e-13 -1.93430704e-13\n",
            " -2.01431762e-13 -2.17952724e-13 -2.54694429e-13 -3.65815399e-13]\n",
            "sol.value(Up) [[-9.88089085e-09 -9.87276826e-09 -9.86405570e-09 ... -5.66719956e-09\n",
            "  -5.99269432e-09 -6.37599201e-09]\n",
            " [-9.85540186e-09 -9.84412863e-09 -9.83186118e-09 ... -3.97226318e-09\n",
            "  -4.44559669e-09 -5.00365947e-09]\n",
            " [-9.88832355e-09 -9.88012807e-09 -9.87129178e-09 ... -6.00378875e-09\n",
            "  -6.31416116e-09 -6.67993851e-09]\n",
            " ...\n",
            " [-9.78308253e-09 -9.76976314e-09 -9.75592576e-09 ... -3.03991010e-09\n",
            "  -3.38948054e-09 -3.81873652e-09]\n",
            " [-9.76483187e-09 -9.74791230e-09 -9.72989821e-09 ...  5.84382455e-10\n",
            "  -1.24783382e-10 -9.61094474e-10]\n",
            " [-9.87139185e-09 -9.86153616e-09 -9.85085013e-09 ... -5.02397733e-09\n",
            "  -5.41457109e-09 -5.87507631e-09]]\n",
            "sol.value(Um) [[ 2.45433729e-03  3.62399036e-08  9.54115557e-09 ...  1.36381976e-09\n",
            "   2.97877500e-09  6.95997288e-09]\n",
            " [ 7.63550440e-04  5.91337451e-08  1.65448859e-08 ...  4.72240334e-09\n",
            "   6.80537459e-09  1.19837501e-08]\n",
            " [ 1.33686885e-05  1.57561233e-09 -4.26046063e-09 ... -6.23364244e-11\n",
            "   1.34599162e-09  4.83926518e-09]\n",
            " ...\n",
            " [-9.23054985e-09 -9.09176453e-09 -8.91491205e-09 ...  2.08774413e-08\n",
            "   2.43709654e-08  3.31472723e-08]\n",
            " [-8.69200630e-09 -8.33973728e-09 -7.81902770e-09 ...  2.29837084e-08\n",
            "   2.76146829e-08  3.88580132e-08]\n",
            " [ 2.01824465e-04  9.62677595e-09 -6.26944471e-10 ...  2.16144612e-09\n",
            "   3.88218782e-09  8.15965499e-09]]\n",
            "Term 1:  -14.769\n",
            "Term 2:  -0.0943919\n",
            "Term 3:  33.6569\n",
            "obj_val:  18.7935\n",
            "norm_log_prob -30.51727269743669\n",
            "log_density_val -1909.87\n",
            "mass matrix used in HMC: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1.]]\n",
            "Term 1:  -14.769\n",
            "Term 2:  -0.231491\n",
            "Term 3:  33.6569\n",
            "obj_val:  18.6564\n",
            "norm_log_prob -30.53850039241617\n",
            "log_density_val -1896.18\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636848260228\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636798785585\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636855885602\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657869\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.51463684435043\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636836008727\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657869\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.51463685538069\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657869\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.5146368453864\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636860171255\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.65787\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636815789537\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657869\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636814668773\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657869\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636788718615\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636818521698\n",
            "log_density_val -1985.1\n",
            "Term 1:  -14.769\n",
            "Term 2:  0.657868\n",
            "Term 3:  33.6569\n",
            "obj_val:  19.5458\n",
            "norm_log_prob -30.514636931824107\n",
            "log_density_val -1985.1\n",
            "\n",
            "=====================================================\n",
            "Started Sampling\n",
            "=====================================================\n",
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "dimension mismatch",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39m# Create MCMC sampler & sample, then calculate diagnostics\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m sampler \u001b[39m=\u001b[39m create_hmc_sampler(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     size\u001b[39m=\u001b[39mgamma_size,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     log_density\u001b[39m=\u001b[39mlog_density,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m     constraint_test\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m gamma_post_samples \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     sample_size\u001b[39m=\u001b[39;49msample_size,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m     initial_state\u001b[39m=\u001b[39;49mgamma_vals,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m gamma_post_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(gamma_post_samples)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/patriciohernandez/Projects/project-amazon/notebooks/HMC.ipynb#X14sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39m# Update ensemble/tracker\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/project-amazon/src/mcmc/hmc.py:793\u001b[0m, in \u001b[0;36mHMCSampler.sample\u001b[0;34m(self, sample_size, verbose, initial_state)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    785\u001b[0m     sample_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    786\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    787\u001b[0m     initial_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    788\u001b[0m ):\n\u001b[1;32m    789\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[39m    Generate and return a sample of size `sample_size`.\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[39m    This method returns a list with each entry representing a sample point from the underlying distribution\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m     hmc_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_MCMC_sampling(\n\u001b[1;32m    794\u001b[0m         sample_size\u001b[39m=\u001b[39;49msample_size,\n\u001b[1;32m    795\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    796\u001b[0m         initial_state\u001b[39m=\u001b[39;49minitial_state,\n\u001b[1;32m    797\u001b[0m     )\n\u001b[1;32m    798\u001b[0m     \u001b[39mreturn\u001b[39;00m hmc_results[\u001b[39m\"\u001b[39m\u001b[39mcollected_ensemble\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/Projects/project-amazon/src/mcmc/hmc.py:1217\u001b[0m, in \u001b[0;36mHMCSampler.start_MCMC_sampling\u001b[0;34m(self, sample_size, initial_state, randomize_step_size, full_diagnostics, verbose)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mfor\u001b[39;00m chain_ind \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(chain_length):\n\u001b[1;32m   1214\u001b[0m     \u001b[39m## Proposal step :propose (momentum, state) pair\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[39m# Generate a momentum proposal\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m     current_momentum \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_white_noise(size\u001b[39m=\u001b[39mstate_space_dimension)\n\u001b[0;32m-> 1217\u001b[0m     current_momentum \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmass_matrix_sqrt_matvec(current_momentum)\n\u001b[1;32m   1219\u001b[0m     \u001b[39m# Advance the current state and momentum to propose a new pair:\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     proposed_momentum, proposed_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_symplectic_integration(\n\u001b[1;32m   1221\u001b[0m         momentum\u001b[39m=\u001b[39mcurrent_momentum,\n\u001b[1;32m   1222\u001b[0m         state\u001b[39m=\u001b[39mcurrent_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         symplectic_integrator\u001b[39m=\u001b[39msymplectic_integrator,\n\u001b[1;32m   1227\u001b[0m     )\n",
            "File \u001b[0;32m~/Projects/project-amazon/src/mcmc/hmc.py:885\u001b[0m, in \u001b[0;36mHMCSampler.mass_matrix_sqrt_matvec\u001b[0;34m(self, momentum)\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    880\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe passed momentum has invalid size;\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    881\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreceived \u001b[39m\u001b[39m{\u001b[39;00mmomentum\u001b[39m}\u001b[39;00m\u001b[39m, expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_CONFIGURATIONS[\u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m     )\n\u001b[1;32m    883\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m\n\u001b[0;32m--> 885\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_MASS_MATRIX_SQRT\u001b[39m.\u001b[39;49mdot(momentum)\n",
            "File \u001b[0;32m~/Projects/project-amazon/venv/lib/python3.10/site-packages/scipy/sparse/_base.py:411\u001b[0m, in \u001b[0;36m_spbase.dot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39m*\u001b[39m other\n\u001b[1;32m    410\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m \u001b[39m@\u001b[39;49m other\n",
            "File \u001b[0;32m~/Projects/project-amazon/venv/lib/python3.10/site-packages/scipy/sparse/_base.py:624\u001b[0m, in \u001b[0;36m_spbase.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    622\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 624\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
            "File \u001b[0;32m~/Projects/project-amazon/venv/lib/python3.10/site-packages/scipy/sparse/_base.py:553\u001b[0m, in \u001b[0;36m_spbase._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    551\u001b[0m     \u001b[39m# dense row or column vector\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N,) \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (N, \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 553\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    555\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(np\u001b[39m.\u001b[39mravel(other))\n\u001b[1;32m    557\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, np\u001b[39m.\u001b[39mmatrix):\n",
            "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
          ]
        }
      ],
      "source": [
        "# Initialize error & iteration counter\n",
        "error = np.infty\n",
        "cntr = 0\n",
        "\n",
        "# Loop until convergence\n",
        "while cntr < max_iter and error > tol:\n",
        "    # Update x0, initial distribution of carbon absorption of the amazon\n",
        "    x0_vals = gamma_vals * forestArea_2017_ha / norm_fac\n",
        "\n",
        "    # Construct Matrix A from new gamma_vals\n",
        "    A[:-2, :] = 0.0\n",
        "    Ax[0:gamma_size] = -alpha * gamma_vals[0:gamma_size]\n",
        "    Ax[-1] = alpha * np.sum(gamma_vals * zbar_2017)\n",
        "    Ax[-2] = -alpha\n",
        "    A[-2, :] = Ax\n",
        "    A[-1, :] = 0.0\n",
        "    A = casadi.sparsify(A)\n",
        "\n",
        "    # Construct Matrix D from new gamma_vals\n",
        "    D[:, :] = 0.0\n",
        "    D[-2, :] = -gamma_vals\n",
        "    D = casadi.sparsify(D)\n",
        "\n",
        "    # Define the right hand side (symbolic here) as a function of gamma\n",
        "    gamma = casadi.MX.sym(\"gamma\", gamma_size + 2)  # state\n",
        "    up = casadi.MX.sym(\"up\", gamma_size)  # control\n",
        "    um = casadi.MX.sym(\"um\", gamma_size)  # control\n",
        "\n",
        "    rhs = (A @ gamma + B @ (up - um) + D @ up) * dt + gamma\n",
        "    f = casadi.Function(\"f\", [gamma, um, up], [rhs])  # dynamics law of motion for state\n",
        "\n",
        "    ## Define an optimizer and initialize it, and set constraints\n",
        "    opti = casadi.Opti()\n",
        "\n",
        "    # Decision variables for states\n",
        "    X = opti.variable(gamma_size + 2, N + 1)\n",
        "\n",
        "    # Aliases for states\n",
        "    Up = opti.variable(gamma_size, N)\n",
        "    Um = opti.variable(gamma_size, N)\n",
        "    Ua = opti.variable(1, N)\n",
        "\n",
        "    # Parameter for initial state\n",
        "    ic = opti.parameter(gamma_size + 2)\n",
        "\n",
        "    # Gap-closing shooting constraints\n",
        "    for k in range(N):\n",
        "        opti.subject_to(X[:, k + 1] == f(X[:, k], Um[:, k], Up[:, k]))\n",
        "\n",
        "    # Initial and terminal constraints\n",
        "    opti.subject_to(X[:, 0] == ic)\n",
        "    opti.subject_to(opti.bounded(0, X[0:gamma_size, :], zbar_2017[0:gamma_size]))\n",
        "\n",
        "    # Objective: regularization of controls\n",
        "    for k in range(gamma_size):\n",
        "        opti.subject_to(opti.bounded(0, Um[k, :], casadi.inf))\n",
        "        opti.subject_to(opti.bounded(0, Up[k, :], casadi.inf))\n",
        "\n",
        "    opti.subject_to(Ua == casadi.sum1(Up + Um) ** 2)\n",
        "\n",
        "    # Set teh optimization problem\n",
        "    term1 = casadi.sum2(ds_vect[0:N, :].T * Ua * zeta / 2)\n",
        "    term2 = -casadi.sum2(ds_vect[0:N, :].T * (pf * (X[-2, 1:] - X[-2, 0:-1])))\n",
        "    term3 = -casadi.sum2(\n",
        "        ds_vect.T * casadi.sum1((pa * theta_vals - pf * kappa) * X[0:gamma_size, :])\n",
        "    )\n",
        "\n",
        "    opti.minimize(term1 + term2 + term3)\n",
        "\n",
        "    # Solve optimization problem\n",
        "    options = dict()\n",
        "    options[\"print_time\"] = False\n",
        "    options[\"expand\"] = True\n",
        "    options[\"ipopt\"] = {\n",
        "        \"print_level\": 0,\n",
        "        \"fast_step_computation\": \"yes\",\n",
        "        \"mu_allow_fast_monotone_decrease\": \"yes\",\n",
        "        \"warm_start_init_point\": \"yes\",\n",
        "    }\n",
        "    opti.solver(\"ipopt\", options)\n",
        "\n",
        "    opti.set_value(\n",
        "        ic,\n",
        "        casadi.vertcat(site_z_vals, np.sum(x0_vals), 1),\n",
        "    )\n",
        "\n",
        "    if _DEBUG:\n",
        "        print(\"ic: \", ic)\n",
        "        print(\"site_z_vals: \", site_z_vals)\n",
        "        print(\"x0_vals: \", x0_vals)\n",
        "        print(\n",
        "            \"casadi.vertcat(site_z_vals,np.sum(x0_vals),1): \",\n",
        "            casadi.vertcat(site_z_vals, np.sum(x0_vals), 1),\n",
        "        )\n",
        "    sol = opti.solve()\n",
        "\n",
        "    if _DEBUG:\n",
        "        print(\"sol.value(X)\", sol.value(X))\n",
        "        print(\"sol.value(Ua)\", sol.value(Ua))\n",
        "        print(\"sol.value(Up)\", sol.value(Up))\n",
        "        print(\"sol.value(Um)\", sol.value(Um))\n",
        "\n",
        "    ## Start Sampling\n",
        "    # Update signature of log density evaluator\n",
        "    log_density = lambda gamma_val: log_density_function(\n",
        "        gamma_val=gamma_val,\n",
        "        gamma_vals_mean=gamma_vals_mean,\n",
        "        theta_vals=theta_vals,\n",
        "        site_precisions=site_precisions,\n",
        "        alpha=alpha,\n",
        "        sol=sol,\n",
        "        X=X,\n",
        "        Ua=Ua,\n",
        "        Up=Up,\n",
        "        zbar_2017=zbar_2017,\n",
        "        forestArea_2017_ha=forestArea_2017_ha,\n",
        "        norm_fac=norm_fac,\n",
        "        alpha_p_Adym=alpha_p_Adym,\n",
        "        Bdym=Bdym,\n",
        "        leng=leng,\n",
        "        T=T,\n",
        "        ds_vect=ds_vect,\n",
        "        zeta=zeta,\n",
        "        xi=xi,\n",
        "        kappa=kappa,\n",
        "        pa=pa,\n",
        "        pf=pf,\n",
        "    )\n",
        "\n",
        "    # Create MCMC sampler & sample, then calculate diagnostics\n",
        "    sampler = create_hmc_sampler(\n",
        "        size=gamma_size,\n",
        "        log_density=log_density,\n",
        "        burn_in=100,\n",
        "        mix_in=2,\n",
        "        symplectic_integrator=\"verlet\",\n",
        "        symplectic_integrator_stepsize=1e-1,\n",
        "        symplectic_integrator_num_steps=3,\n",
        "        constraint_test=lambda x: True if np.all(x >= 0) else False,\n",
        "    )\n",
        "    gamma_post_samples = sampler.sample(\n",
        "        sample_size=sample_size,\n",
        "        initial_state=gamma_vals,\n",
        "        verbose=True,\n",
        "    )\n",
        "    gamma_post_samples = np.asarray(gamma_post_samples)\n",
        "\n",
        "    # Update ensemble/tracker\n",
        "    collected_ensembles.update({cntr: gamma_post_samples.copy()})\n",
        "\n",
        "    # Update gamma value\n",
        "    weight = 0.25  # <-- Not sure how this linear combination weighting helps!\n",
        "    if mode_as_solution:\n",
        "        raise NotImplementedError(\n",
        "            \"We will consider this in the future; trace sampled points and keep track of objective values to pick one with highest prob. \"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        gamma_vals = (\n",
        "            weight * np.mean(gamma_post_samples, axis=0) + (1 - weight) * gamma_vals_old\n",
        "        )\n",
        "    gamma_vals_tracker.append(gamma_vals.copy())\n",
        "\n",
        "    # Evaluate error for convergence check\n",
        "    error = np.max(np.abs(gamma_vals_old - gamma_vals) / gamma_vals_old)\n",
        "    error_tracker.append(error)\n",
        "    print(f\"Iteration [{cntr+1:4d}]: Error = {error}\")\n",
        "\n",
        "    # Exchange gamma values (for future weighting/update & error evaluation)\n",
        "    gamma_vals_old = gamma_vals\n",
        "\n",
        "    # Increase the counter\n",
        "    cntr += 1\n",
        "\n",
        "    results.update(\n",
        "        {\n",
        "            \"cntr\": cntr,\n",
        "            \"error_tracker\": np.asarray(error_tracker),\n",
        "            \"gamma_vals_tracker\": np.asarray(gamma_vals_tracker),\n",
        "            \"collected_ensembles\": collected_ensembles,\n",
        "        }\n",
        "    )\n",
        "    pickle.dump(results, open(\"results.pcl\", \"wb\"))\n",
        "\n",
        "    # Extensive plotting for monitoring; not needed really!\n",
        "    if False:\n",
        "        plt.plot(gamma_vals_tracker[-2], label=r\"Old $\\gamma$\")\n",
        "        plt.plot(gamma_vals_tracker[-1], label=r\"New $\\gamma$\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        for j in range(gamma_size):\n",
        "            plt.hist(gamma_post_samples[:, j], bins=50)\n",
        "            plt.title(f\"Iteration {cntr}; Site {j+1}\")\n",
        "            plt.show()\n",
        "\n",
        "print(\"Terminated. Sampling the final distribution\")\n",
        "# Sample (densly) the final distribution\n",
        "final_sample = sampler.sample(\n",
        "    sample_size=final_sample_size,\n",
        "    initial_state=gamma_vals,\n",
        "    verbose=True,\n",
        ")\n",
        "final_sample = np.asarray(final_sample)\n",
        "results.update({\"final_sample\": final_sample})\n",
        "pickle.dump(results, open(\"results.pcl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4P_aPkTYC50"
      },
      "source": [
        "## 3.4 Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgLSV--_YHI3"
      },
      "outputs": [],
      "source": [
        "# Plot Error Results\n",
        "plt.plot(results[\"error_tracker\"])\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-G5CN98YHt3"
      },
      "outputs": [],
      "source": [
        "# Plot Gamma Estimate Update\n",
        "for j in range(results[\"gamma_size\"]):\n",
        "    plt.plot(results[\"gamma_vals_tracker\"][:, j], label=r\"$\\gamma_{%d}$\" % (j + 1))\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6c4VH3kYJT6"
      },
      "outputs": [],
      "source": [
        "# Plot Gamma Estimate Update\n",
        "for j in range(results[\"gamma_size\"]):\n",
        "    plt.plot(results[\"gamma_vals_tracker\"][:, j], label=r\"$\\gamma_{%d}$\" % (j + 1))\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUVpwYgiYL50"
      },
      "outputs": [],
      "source": [
        "# Plot Histograms\n",
        "for itr in results[\"collected_ensembles\"].keys():\n",
        "    for j in range(results[\"gamma_size\"]):\n",
        "        plt.hist(results[\"collected_ensembles\"][itr][:, j], bins=100)\n",
        "        plt.title(f\"Iteration {itr+1}; Site {j+1}\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYV6IIGHYN8h"
      },
      "outputs": [],
      "source": [
        "# Plot Histogram of the final sample\n",
        "for j in range(results[\"gamma_size\"]):\n",
        "    plt.hist(results[\"final_sample\"][:, j], bins=100)\n",
        "    plt.title(f\"Final Sample; Site {j+1}\")\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
